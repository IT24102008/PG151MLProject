{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Pipeline for Stroke Prediction Data Preprocessing and Feature Engineering\n",
    "This notebook integrates all the individual data preprocessing and feature engineering steps into a single, cohesive pipeline. It covers handling missing data, encoding categorical variables, outlier removal, normalization/scaling, feature selection, and dimensionality reduction using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T11:39:27.876315Z",
     "iopub.status.busy": "2025-09-21T11:39:27.876230Z",
     "iopub.status.idle": "2025-09-21T11:39:29.751766Z",
     "shell.execute_reply": "2025-09-21T11:39:29.750499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before imputation:\n",
      "id                     0\n",
      "gender                 0\n",
      "age                    0\n",
      "hypertension           0\n",
      "heart_disease          0\n",
      "ever_married           0\n",
      "work_type              0\n",
      "Residence_type         0\n",
      "avg_glucose_level      0\n",
      "bmi                  201\n",
      "smoking_status         0\n",
      "stroke                 0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "id                   0\n",
      "gender               0\n",
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "ever_married         0\n",
      "work_type            0\n",
      "Residence_type       0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n",
      "\n",
      "Data after outlier removal:\n",
      "\n",
      "Data after scaling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hr/25jg81zj1jv0crwfx5c5q80c0000gn/T/ipykernel_71209/2829217464.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['bmi'].fillna(bmi_mean, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features: ['age', 'bmi', 'avg_glucose_level', 'gender_Male', 'Residence_type_Urban', 'hypertension', 'smoking_status_never smoked', 'work_type_Private', 'heart_disease', 'smoking_status_formerly smoked', 'work_type_Self-employed', 'smoking_status_smokes', 'ever_married_Yes']\n",
      "\n",
      "Shape after PCA: (5110, 10)\n",
      "\n",
      "Final preprocessed dataframe head:\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  2.305606  1.264816  0.250546 -0.430617  0.834524  0.333307  0.320894   \n",
      "1  1.453986  1.647613 -0.484745  0.160239 -1.163737 -0.412336 -0.231320   \n",
      "2  1.571330 -0.416363 -0.638432  0.597845  0.169697 -0.612380  0.748779   \n",
      "3  1.537974  1.474840  0.641771  0.057266  0.514872  0.671681 -0.587822   \n",
      "4  1.642192  1.709071 -1.520156  0.258872 -1.103096 -0.424797 -0.188216   \n",
      "\n",
      "          7         8         9  stroke  \n",
      "0 -0.643058 -0.062396 -0.212238       1  \n",
      "1  0.114952  0.208955 -0.247852       1  \n",
      "2  0.043821 -0.296528 -0.280570       1  \n",
      "3  0.692612  0.170372 -0.125929       1  \n",
      "4  0.153761 -0.201548  0.639121       1  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset once at the beginning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = '../data/raw/StrokeData.csv'  # Use relative path from notebooks directory\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 1: Handling missing BMI data (mean imputation)\n",
    "print('\\nMissing values before imputation:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "bmi_mean = df['bmi'].mean()\n",
    "df['bmi'].fillna(bmi_mean, inplace=True)\n",
    "\n",
    "print('\\nMissing values after imputation:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Encoding categorical variables (one-hot encoding)\n",
    "categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_encoded.drop(['id'], axis=1, inplace=True)  # Assuming id is not needed\n",
    "\n",
    "# Step 3: Outlier removal (IQR method, before scaling)\n",
    "numerical_cols = ['age', 'avg_glucose_level', 'bmi']\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_encoded[col].quantile(0.25)\n",
    "    Q3 = df_encoded[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_encoded[col] = np.clip(df_encoded[col], lower_bound, upper_bound)\n",
    "\n",
    "print('\\nData after outlier removal:')\n",
    "\n",
    "# Step 4: Normalization/Scaling (after outlier removal)\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "print('\\nData after scaling:')\n",
    "\n",
    "# Step 5: Feature Selection (using RandomForest)\n",
    "X = df_encoded.drop('stroke', axis=1)\n",
    "y = df_encoded['stroke']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "selected_features = importances[importances > 0.01].index.tolist()\n",
    "\n",
    "print('\\nSelected features:', selected_features)\n",
    "\n",
    "# Step 6: Dimension Reduction (PCA on selected features)\n",
    "X_selected = X[selected_features]\n",
    "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_pca = pca.fit_transform(X_selected)\n",
    "\n",
    "print('\\nShape after PCA:', X_pca.shape)\n",
    "\n",
    "# Final preprocessed data\n",
    "df_final = pd.DataFrame(X_pca)\n",
    "df_final['stroke'] = y.values\n",
    "\n",
    "print('\\nFinal preprocessed dataframe head:')\n",
    "print(df_final.head())\n",
    "\n",
    "# Save the final dataset\n",
    "df_final.to_csv('../results/outputs/preprocessed_stroke_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
